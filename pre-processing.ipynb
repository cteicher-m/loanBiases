{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek to answer the question:\n",
    "### Is lending racially discriminatory in the US?\n",
    "\n",
    "Before we look for loan biases in this housing data we must import and clean the data set so that we can perform analyses. We will utilize our data dictionary in order to understand the value of each column.\n",
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import hashlib\n",
    "from functools import reduce\n",
    "import enum\n",
    "\n",
    "# Get the total number of rows in the data set prior to filtering out bad, missing, or corrupt lines\n",
    "# use the number to compare the size of the data set after filtering \n",
    "columnNames = []\n",
    "with open('headers.txt', 'r') as headerFile:\n",
    "    headerReader = csv.reader(headerFile, delimiter=',')\n",
    "    for row in headerReader:\n",
    "        columnNames.append(row[1])\n",
    "        \n",
    "numCols = len(columnNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the data dictionary in this repository, https://www.ffiec.gov/hmda/glossary.htm contains explanations of columns and acronyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out clean lines to new csv\n",
    "invalidCols = 0; duplicateRows = 0; keptRows = 0; missingCols = 0; totalRows = 0\n",
    "onHeader = True\n",
    "rows = set()\n",
    "with open('hmda_lar.csv', 'r') as dataFile:\n",
    "    with open('valid_rows_sample_small.csv', 'w') as outFile:\n",
    "        dataReader = csv.reader(dataFile, delimiter=',')\n",
    "        outWriter = csv.writer(outFile, delimiter = ',')\n",
    "        for row in dataReader:\n",
    "            # Skip the header line\n",
    "            totalRows += 1\n",
    "            # Ignore rows with incorrect number of columns\n",
    "            if len(row) != numCols:\n",
    "                invalidCols += 1\n",
    "                continue \n",
    "            else:\n",
    "                # Ignore rows where more than 1/2 of the entries are missing\n",
    "                # Count the number of nan's in a row\n",
    "                missingFields = reduce(lambda x, y: x + int(y == \"\"), row, 0) # do not change \"\" to ''\n",
    "                if missingFields >= int(0.5 * numCols):\n",
    "                    missingCols += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    keptRows += 1\n",
    "                    outWriter.writerow(row)\n",
    "print(\"Dropped: %d Missing: %d   Kept: %d   Total: %d\" % (invalidCols, missingCols,\n",
    "                                                                             keptRows, totalRows))\n",
    "\n",
    "# If we only drop duplicates that match on all fields these are the results.   \n",
    "# Dropped:     Duplicates:     De-duplicated:     Total:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in new csv with clean lines\n",
    "df_dup = pd.read_csv(\"valid_rows_sample_small.csv\", sep=',', engine='python', error_bad_lines=False, dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of rows dropped in new csv\n",
    "df_dedup = df_dup.drop_duplicates(keep='first');\n",
    "duplicateRows = df_dup.shape[0]- df_dedup.shape[0]\n",
    "print(\"Duplicates: %d\" % duplicateRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarantees all rows are accounted for after filtering data\n",
    "invalidCols + duplicateRows + missingCols + keptRows == totalRows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following object contains suggested data types for the corresponding columns. The column headers not in this object are best represented as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colToType = {\n",
    "    \"tract_to_msamd_income\" : float, \n",
    "    \"rate_spread\" : float,\n",
    "    \"population\" : int,\n",
    "    \"minority_population\" : bool,\n",
    "    \"number_of_owner_occupied_units\" : int, \n",
    "    \"number_of_1_to_4_family_units\" : int, \n",
    "    \"loan_amount_000s\" : float, \n",
    "    \"hud_median_family_income\" : float,\n",
    "    \"applicant_income_000s\" : float,\n",
    "    \"sequence_number\" : int, \n",
    "    \"census_tract_number\" : float, \n",
    "    \"as_of_year\" : int,\n",
    "    \"application_date_indicator\" : int,     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column types\n",
    "df_test = df_dedup\n",
    "# Use Pandas drop_duplicates() as evidence that dataset is deduplicated\n",
    "print(\"Deduplicated Valid Rows: %d\\tFully Deduplicated: %r\" \n",
    "      % (len(df_test), len(df_test) == len(df_test.drop_duplicates())))\n",
    "print(\"Columns: %d\" % len(df_test.columns.values))\n",
    "\n",
    "# Convert types of columns\n",
    "for colName, colType in colToType.items():\n",
    "    if colType == int:\n",
    "        df_test[colName] = df_test[colName].apply(lambda x: x if x != 'nan' else 0).astype(int)\n",
    "    if colType == float:\n",
    "        df_test[colName] = df_test[colName].apply(lambda x: x if x != 'nan' else float('nan')).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some fields may have values that are incompatible types. This may occur when no data is stored for a variable, a user did not complete the application, or a column may contain multiple data types. A string representation of an age cannot be compared to a number. If a user inputted N/A, or left that field blank, it is interpreted differently as NA, na, NaN. In this data set, missing information is encoded as \"Information not provided by applicant in mail, Internet, or telephone application\" as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace inconsistent empty row entries \n",
    "df_test.replace(\"nan\", np.nan, inplace=True)\n",
    "df_test.replace(\"None\", np.nan, inplace=True)\n",
    "df_test.replace(\"Not applicable\", np.nan, inplace = True)\n",
    "df_test.replace(\"Information not provided by applicant in mail, Internet, or telephone application\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data set is specific to New York State in 2015 so there is no need to keep the state name, year, and abbrevation NY\n",
    "df_test.drop([\"state_name\",\"state_abbr\", \"as_of_year\"],axis=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to look at the counts for different columns. This will help us eliminate columns that are not well populated or rows that are too empty. Each grouping is on the sequence number, a unique indentifier for each application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('applicant_sex_name').sequence_number.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are twice as many male applicants (or at least applicants with commonly male names) in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('lien_status_name').sequence_number.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lien status is an indication of expected default and collateral. It guarantees an underlying obligation, where if not satisfied, a creditor can seize the asset that is the subject of the lien. \n",
    "\n",
    "Get rid of: application withdrawn, file closed for incompleteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_test.groupby('action_taken_name').sequence_number.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bank loan that gets approved is considered \"originated\" and is indicated under the \"action_taken_name\" column. A loan may not originated due to 1 of 6 options: the loan application was approved but not accepted, application denied by financial institution, application withdrawn by applicant, file closed for incompleteness, loan purchased by the institution, preapproval request denied by finanical institution. We are only interested in analyzing if a loan application was submitted and if that application was approved or not approved. Therefore, we can remove columns that provide additional information about action taken following a loan that was not approved, or if an application was not completed/withdrawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(df_test[df_test.action_taken_name == \"Application withdrawn by applicant\"].index)\n",
    "df_test = df_test.drop(df_test[df_test.action_taken_name == \"File closed for incompleteness\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('action_taken_name').sequence_number.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only looking at loans that were originated or purchased by the institution. All other types of action names are conisdered rejections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the different reasons, up to 3 per application, for why a loan was not originated\n",
    "df_test.groupby('denial_reason_name_1').apply(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.groupby('agency_abbr').sequence_number.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see 6 different agencies can approve loan applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('property_type_name').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there is no income data on multifamily dwellings, so we will drop these row entries. 98.2% of the data is with respect to one-to-four-family dwellings, so we will also drop rows with maufactured housing. Now all remaining row entries are for one-to-four-family dwellings. We can drop this column overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby(\"loan_type_name\").sequence_number.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(df_test[df_test.property_type_name == \"Multifamily dwelling\"].index)\n",
    "df_test = df_test.drop(df_test[df_test.property_type_name == \"Manufactured housing\"].index)\n",
    "df_test.drop([\"property_type_name\"],axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('preapproval_name').sequence_number.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('loan_purpose_name').sequence_number.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A majority of applications were loan requests needed to purchase a home. However, racial discrimination may be prevalent in any type of loan so we will not distinguish between loan types in the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('co_applicant_race_name_5').sequence_number.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('co_applicant_race_name_4').sequence_number.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('co_applicant_race_name_3').sequence_number.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('co_applicant_race_name_2').sequence_number.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('co_applicant_race_name_1').sequence_number.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop([\"co_applicant_race_name_2\", \"co_applicant_race_name_3\", \"co_applicant_race_name_4\", \"co_applicant_race_name_5\"],axis=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "co-applicant_race_name[2-5] are nearly blank on most row entries. We can delete these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"0\" means the application was made on or after 1/1/2004 and \"2\" means the application date is not available. Because we are only looking at one year, 2015, we will drop this column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process this data and model trends in loan biases, we will only work with numeric entries. Therefore, we must encode categorical columns with numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('application_date_indicator').sequence_number.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(['application_date_indicator'],axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodes categorical to numerical types needed for processing\n",
    "def encode_action(action_type, category):\n",
    "    if action_type == category:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encode = df_test.copy()\n",
    "df_encode.action_taken_name = df_encode.action_taken_name.apply(lambda x: \n",
    "     int(x in ['Loan originated', 'Loan purchased by the institution']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will bucket income into the standard US tax brackets found at https://web.blockadvisors.com/2017-tax-brackets/ in order to control for income and consider the impact of race on loan status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['applicant_income_000s'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the max applicant income reported is 9999 thousand \n",
    "print(\"Number of applicants with reported income above $9.9 million:\", \n",
    "      df_test[df_test['applicant_income_000s'] == 9999.000000].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at distribution of incomes\n",
    "df_test['loan_amount_000s'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the max loan amount requested is 99999 thousand which are probably outliers or reporting errors\n",
    "print(\"Number of applicants with loan requested above $99.9 million:\", \n",
    "      df_test[df_test['loan_amount_000s'] == 99999.000000].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not provide the maximum bucket value, all incomes that do not fall within these specific \n",
    "# categories will be reported as NaN, so 9999 is the highest possible value.\n",
    "df_encode['income_bracket'] = pd.cut(df_test['applicant_income_000s'], [0, 18, 75, 153, 233, 416, 470, 9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encode.groupby(\"rate_spread\").sequence_number.describe();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rate spread which is the difference between the loan's annual percentage rate (APR) and the average prime offer rate (APOR), is a good indicator of bias/discriminatory lending. A higher-priced mortgage loan is a consumer credit transaction secured by the consumer’s principal dwelling with an annual percentage rate (APR) that exceeds the average prime offer rate (APOR) by a given amount. In general, for a first-lien mortgage, a loan is “higher-priced” if its APR exceeds the APOR by 1.5 percent or more. For a subordinate mortgage, a loan is “higher-priced” if its APR exceeds the APOR by 3.5 percent. https://www.scotsmanguide.com/Residential/Articles/2014/11/High-Cost-vs--Higher-Priced-Mortgages/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download as csv for processing notebook\n",
    "df_encode.to_csv(\"encoded_loan_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
