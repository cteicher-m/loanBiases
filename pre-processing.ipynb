{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek to answer the question:\n",
    "### Is algorithmic lending racially biased? <font color='red'>But let's change this research question</font> \n",
    "\n",
    "\n",
    "Before we look for loan biases in this housing data we must import and clean the data set so that we can perform analyses. <font color='red'>We need a data dictionary, I have found some that aren't great and don't totally match our data set but are released by HMDA affiliates.</font> \n",
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import hashlib\n",
    "from functools import reduce\n",
    "import enum\n",
    "\n",
    "# Get the total number of rows in the data set prior to filtering out bad, missing, or corrupt lines\n",
    "# use the number to compare the size of the data set after filtering \n",
    "columnNames = []\n",
    "with open('headers.txt', 'r') as headerFile:\n",
    "    headerReader = csv.reader(headerFile, delimiter=',')\n",
    "    for row in headerReader:\n",
    "        columnNames.append(row[1])\n",
    "        \n",
    "numCols = len(columnNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ffiec.gov/hmda/glossary.htm contains explanations of many columns and acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalidCols = 0; duplicateRows = 0; keptRows = 0; missingCols = 0; totalRows = 0\n",
    "onHeader = True\n",
    "rows = set()\n",
    "with open('hmda_lar.csv', 'r') as dataFile:\n",
    "    with open('valid_rows_sample_small.csv', 'w') as outFile:\n",
    "        dataReader = csv.reader(dataFile, delimiter=',')\n",
    "        outWriter = csv.writer(outFile, delimiter = ',')\n",
    "        for row in dataReader:\n",
    "            # Skip the header line\n",
    "            totalRows += 1\n",
    "            # Ignore rows with incorrect number of columns\n",
    "            if len(row) != numCols:\n",
    "                invalidCols += 1\n",
    "                continue \n",
    "            else:\n",
    "                # Ignore rows where more than 1/2 of the entries are missing\n",
    "                # Count the number of nan's in a row\n",
    "                missingFields = reduce(lambda x, y: x + int(y == \"\"), row, 0) # do not change \"\" to ''\n",
    "                if missingFields >= int(0.5 * numCols):\n",
    "                    missingCols += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    keptRows += 1\n",
    "                    outWriter.writerow(row)\n",
    "print(\"Dropped: %d Missing: %d   Kept: %d   Total: %d\" % (invalidCols, missingCols,\n",
    "                                                                             keptRows, totalRows))\n",
    "\n",
    "# If we only drop duplicates that match on all fields these are the results.   \n",
    "# Dropped:     Duplicates:     De-duplicated:     Total:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dup = pd.read_csv(\"valid_rows_sample_small.csv\", sep=',', engine='python', error_bad_lines=False, dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dedup = df_dup.drop_duplicates(keep='first');\n",
    "duplicateRows = df_dup.shape[0]- df_dedup.shape[0]\n",
    "print(\"Duplicates: %d\" % duplicateRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarantees all rows are accounted for after filtering data\n",
    "invalidCols + duplicateRows + missingCols + keptRows == totalRows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following object contains suggested data types for the corresponding columns. The column headers not in this object are best represented as strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colToType = {\n",
    "    \"tract_to_msamd_income\" : float, \n",
    "    \"rate_spread\" : float,\n",
    "    \"population\" : int,\n",
    "    \"minority_population\" : bool,\n",
    "    \"number_of_owner_occupied_units\" : int, \n",
    "    \"number_of_1_to_4_family_units\" : int, \n",
    "    \"loan_amount_000s\" : float, \n",
    "    \"hud_median_family_income\" : float,\n",
    "    \"applicant_income_000s\" : float,\n",
    "    \"sequence_number\" : int, \n",
    "    \"census_tract_number\" : float, \n",
    "    \"as_of_year\" : int,\n",
    "    \"application_date_indicator\" : int,     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_dedup\n",
    "# Use Pandas drop_duplicates() as evidence that dataset is deduplicated\n",
    "print(\"Deduplicated Valid Rows: %d\\tFully Deduplicated: %r\" \n",
    "      % (len(df_test), len(df_test) == len(df_test.drop_duplicates())))\n",
    "print(\"Columns: %d\" % len(df_test.columns.values))\n",
    "\n",
    "# Convert types of columns\n",
    "for colName, colType in colToType.items():\n",
    "    if colType == int:\n",
    "        df_test[colName] = df_test[colName].apply(lambda x: x if x != 'nan' else 0).astype(int)\n",
    "    if colType == float:\n",
    "        df_test[colName] = df_test[colName].apply(lambda x: x if x != 'nan' else float('nan')).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some fields may have values that are incompatible types. This may occur when no data is stored for a variable, a user did not complete the application, or a column may contain multiple data types. A string representation of an age cannot be compared to a number. If a user inputted N/A, or left that field blank, it is interpreted differently as NA, na, NaN. In this data set, missing information is encoded as \"Information not provided by applicant in mail, Internet, or telephone application\" as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.replace(\"nan\", np.nan, inplace=True)\n",
    "df_test.replace(\"None\", np.nan, inplace=True)\n",
    "df_test.replace(\"Information not provided by applicant in mail, Internet, or telephone application\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this data set is specific to New York State in 2015 so there is no need to keep the state name, year, and abbrevation NY\n",
    "df_test.drop([\"state_name\",\"state_abbr\", \"as_of_year\"],axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('lien_status_name').respondent_id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get rid of: application withdrawn, file closed for incompleteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df_test.action_taken_name.values).unique()\n",
    "df_test.groupby('action_taken_name').respondent_id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bank loan that gets approved is considered \"originated\" and is indicated under the \"action_taken_name\" column. A loan may not originated due to 1 of 6 options: the loan application was approved but not accepted, application denied by financial institution, application withdrawn by applicant, file closed for incompleteness, loan purchased by the institution, preapproval request denied by finanical institution. We are only interested in analyzing if a loan application was submitted and if that application was approved or not approved. Therefore, we can remove columns that provide additional information about action taken following a loan that was not approved, or if an application was not completed/withdrawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(df_test[df_test.action_taken_name == \"Application withdrawn by applicant\"].index)\n",
    "df_test = df_test.drop(df_test[df_test.action_taken_name == \"File closed for incompleteness\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('action_taken_name').respondent_id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the different reasons up to 3 per application for why a loan was not originated\n",
    "df_test.groupby('denial_reason_name_1').apply(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add back in if we get rid of denial reasons and only focus on loan originated/not loan originated\n",
    "# Recording reasons for denial is optional, except for institutions supervised by the Office of Thrift Supervision (OTS)* or the Office of the Comptroller of the Currency (OCC).\n",
    "#df_test.drop([\"denial_reason_name_1\",\"denial_reason_name_2\", \"denial_reason_name_3\"],axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.groupby('agency_abbr').respondent_id.count()#apply(lambda x: x.nunique())\n",
    "# 6 different agencies can approve these loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>ADD: We should drop more that goes beyond approved/not approved. Thoughts: lien status is for collateral- is this too specific? </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process this data and model trends in loan biases, we will only work with numeric entries. Therefore, we must encode categorical columns with numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential columns be encoded: \n",
    "# action_taken_name: 0 originated, 1 not originated\n",
    "# agency_name: \n",
    "# action_taken: \n",
    "# applicant_race_name_1:\n",
    "# applicant_race_name_2:\n",
    "# applicant_race_name_3:\n",
    "# applicant_race_name_4: \n",
    "# applicant_race_name_5: \n",
    "# applicant_sex_name: 0 male, 1 female\n",
    "# co_applicant_ethnicity_name:\n",
    "# co_applicant_race_name_1:\n",
    "# co_applicant_race_name_1:\n",
    "# co_applicant_race_name_1:\n",
    "# co_applicant_sex_name:\n",
    "# county_name:\n",
    "# hoepa_status_name:\n",
    "# lien_status_name:\n",
    "# loan_purpose_name:\n",
    "# loan_type_name:\n",
    "# purchaser_type_name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical to numerical for processing\n",
    "def encode_action(action_type, category):\n",
    "    if action_type == category:\n",
    "        return 0\n",
    "    else: \n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encode = df_test.copy()\n",
    "df_encode.action_taken_name = df_encode.action_taken_name.apply(lambda x: encode_action(x, 'Loan originated'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will bucket income into the standard US tax brackets found at https://web.blockadvisors.com/2017-tax-brackets/ in order to control for income and consider the impact of race on loan status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['applicant_income_000s'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the max applicant income reported is 9999 thousand and there are 37 applicants with this income. \n",
    "print(\"Number of applicants with reported income above $9.9 million:\", \n",
    "      df_test[df_test['applicant_income_000s'] == 9999.000000].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you do not provide the maximum bucket value, all incomes that do not fall within these specific categories will be reported as NaN\n",
    "df_encode['income_bracket'] = pd.cut(df_test['applicant_income_000s'], [0, 18, 75, 153, 233, 416, 470, 9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download as csv for processing notebook \n",
    "df_encode.to_csv(\"encoded_loan_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
